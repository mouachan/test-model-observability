# Test Model Observability

Ce d√©p√¥t contient les scripts de test pour valider le fonctionnement de Llama Stack avec l'observabilit√© et le tracing distribu√©.

## üìã √Ä propos

Ces tests sont con√ßus pour fonctionner avec le projet [lls-observability](https://github.com/rh-ai-quickstart/lls-observability), un quickstart Red Hat qui d√©ploie une infrastructure compl√®te d'observabilit√© pour les applications AI sur OpenShift AI.

**D√©p√¥t de base:** [rh-ai-quickstart/lls-observability](https://github.com/rh-ai-quickstart/lls-observability)

## üöÄ Pr√©requis

Avant d'ex√©cuter ces tests, vous devez avoir d√©ploy√© l'infrastructure d'observabilit√© et les services AI :

1. **D√©ployer le stack complet** selon les instructions du [d√©p√¥t principal](https://github.com/rh-ai-quickstart/lls-observability)
2. **V√©rifier que les services sont disponibles** :
   ```bash
   # V√©rifier Llama Stack
   oc get svc llama-stack-instance-service -n llama-serve
   
   # V√©rifier Llama Guard
   oc get svc llama-guard-3-1b-predictor -n llama-serve
   
   # V√©rifier otel-collector
   oc get opentelemetrycollector otel-collector -n observability-hub
   ```

## üì¶ Installation

```bash
# Cloner ce d√©p√¥t
git clone https://github.com/mouachan/test-model-observability.git
cd test-model-observability

# Installer les d√©pendances
pip install -r requirements.txt
```

## üß™ Tests disponibles

### 1. Test Multimodal - Extraction de ticket de caisse

Ce test utilise un mod√®le multimodal pour extraire les produits et leurs prix d'une image de ticket de caisse.

**Pr√©paration:**
Si vous avez une URL d'image, vous pouvez la t√©l√©charger d'abord:
```bash
python download_receipt_image.py <url_de_l_image> receipt.jpg
```

**Usage:**
```bash
python test_multimodal_receipt.py <chemin_vers_image>
```

**Exemple:**
```bash
# T√©l√©charger l'image depuis une URL (optionnel)
python download_receipt_image.py https://example.com/receipt.jpg receipt.jpg

# Analyser l'image
python test_multimodal_receipt.py receipt.jpg
```

**Fonctionnalit√©s:**
- Analyse d'image avec mod√®le multimodal
- Extraction structur√©e des produits et prix
- G√©n√©ration de JSON avec les r√©sultats
- Traces OpenTelemetry pour l'observabilit√©

**R√©sultats:**
- Affiche les produits extraits avec leurs prix
- Sauvegarde les r√©sultats dans `receipt_extraction_result.json`
- Envoie les traces √† Tempo via otel-collector

**Note:** Ce script n√©cessite un mod√®le multimodal compatible avec les images. Si votre mod√®le ne supporte pas directement les images, vous devrez peut-√™tre utiliser un service OCR pr√©alable ou adapter le script.

### 2. Test Llama avec Guardrails

Ce test v√©rifie que Llama Guard fonctionne correctement pour filtrer les r√©ponses non s√©curis√©es.

**Usage:**
```bash
python test_llama_guardrails.py
```

**Fonctionnalit√©s:**
- G√©n√©ration de r√©ponses avec Llama Stack
- V√©rification avec Llama Guard pour chaque r√©ponse
- Tests avec diff√©rents types de prompts (s√ªrs et potentiellement probl√©matiques)
- Rapport d√©taill√© des r√©sultats

**R√©sultats:**
- Affiche chaque test avec le statut SAFE/UNSAFE
- Sauvegarde les r√©sultats dans `guardrails_test_results.json`
- Envoie les traces √† Tempo via otel-collector

## üîß Configuration

Les scripts utilisent des variables d'environnement avec des valeurs par d√©faut pour se connecter aux services d√©ploy√©s dans le cluster :

### Variables d'environnement

| Variable | Valeur par d√©faut | Description |
|----------|-------------------|-------------|
| `LLAMA_STACK_URL` | `http://llama-stack-instance-service.llama-serve.svc.cluster.local:8321` | URL du service Llama Stack |
| `LLAMA_GUARD_URL` | `http://llama-guard-3-1b-predictor.llama-serve.svc.cluster.local/v1` | URL du service Llama Guard |
| `MODEL_NAME` | `meta-llama/Llama-3.2-3B-Instruct` | Nom du mod√®le √† utiliser |
| `OTEL_TRACE_ENDPOINT` | `http://otel-collector-collector.observability-hub.svc.cluster.local:4318/v1/traces` | Endpoint OTLP pour les traces |

### Personnalisation

Pour utiliser des endpoints diff√©rents, d√©finissez les variables d'environnement avant d'ex√©cuter les tests :

```bash
export LLAMA_STACK_URL="http://votre-service:8321"
export LLAMA_GUARD_URL="http://votre-guard-service/v1"
export MODEL_NAME="votre-modele"
python test_multimodal_receipt.py receipt.jpg
```

## üìä Traces OpenTelemetry

Les deux scripts envoient automatiquement les traces √† l'otel-collector configur√©. Vous pouvez visualiser les traces dans:

1. **OpenShift Console** ‚Üí Observe ‚Üí Traces
2. **Grafana** ‚Üí Tempo datasource

Les traces incluent:
- Dur√©e des requ√™tes
- Statut des r√©ponses (succ√®s/erreur)
- M√©tadonn√©es sur les mod√®les utilis√©s
- Informations sur les guardrails appliqu√©s
- Spans d√©taill√©s pour chaque √©tape du processus

### Exemple de visualisation

Apr√®s avoir ex√©cut√© les tests, vous pouvez :

1. Ouvrir OpenShift Console
2. Naviguer vers **Observe** ‚Üí **Traces**
3. Filtrer par service :
   - `multimodal-receipt-extractor` pour le test multimodal
   - `llama-guardrails-test` pour le test guardrails
4. Examiner les spans d√©taill√©s de chaque requ√™te

## üê≥ Ex√©cution depuis un Pod dans le cluster

Pour ex√©cuter les tests depuis un pod dans le cluster (recommand√© pour √©viter les probl√®mes de r√©seau) :

### Option 1: Pod temporaire avec Python

```bash
# Cr√©er un pod temporaire
oc run test-pod \
  --image=python:3.11 \
  --rm -it \
  --restart=Never \
  --namespace=llama-serve \
  -- sh

# Dans le pod, installer les d√©pendances
pip install requests opentelemetry-api opentelemetry-sdk opentelemetry-exporter-otlp-proto-http

# Copier les scripts (depuis votre machine locale)
# ou cloner le d√©p√¥t dans le pod
git clone https://github.com/mouachan/test-model-observability.git
cd test-model-observability
pip install -r requirements.txt

# Ex√©cuter les tests
python test_llama_guardrails.py
python test_multimodal_receipt.py <chemin_image>
```

### Option 2: Job Kubernetes

Cr√©ez un Job Kubernetes pour ex√©cuter les tests :

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: test-model-observability
  namespace: llama-serve
spec:
  template:
    spec:
      containers:
      - name: test
        image: python:3.11
        command:
        - /bin/sh
        - -c
        - |
          pip install -r requirements.txt
          python test_llama_guardrails.py
        volumeMounts:
        - name: test-scripts
          mountPath: /tests
      volumes:
      - name: test-scripts
        configMap:
          name: test-scripts
      restartPolicy: Never
```

## üîç D√©pannage

### Erreur de connexion √† Llama Stack

**Sympt√¥mes:** `Connection refused` ou `Name resolution failed`

**Solutions:**
- V√©rifier que le service `llama-stack-instance-service` est disponible dans le namespace `llama-serve`
- V√©rifier les routes et les services: `oc get svc,route -n llama-serve`
- Si vous ex√©cutez depuis l'ext√©rieur du cluster, utilisez la route publique :
  ```bash
  export LLAMA_STACK_URL="https://$(oc get route llama-stack-instance-service -n llama-serve -o jsonpath='{.spec.host}')"
  ```

### Erreur de connexion √† Llama Guard

**Sympt√¥mes:** `Connection refused` ou erreur 404

**Solutions:**
- V√©rifier que le service `llama-guard-3-1b-predictor` est disponible
- V√©rifier les logs: `oc logs -n llama-serve -l app=llama-guard`
- V√©rifier que l'InferenceService est pr√™t: `oc get inferenceservice llama-guard-3-1b -n llama-serve`

### Traces non visibles dans Tempo

**Sympt√¥mes:** Les traces n'apparaissent pas dans l'interface

**Solutions:**
- V√©rifier que l'otel-collector est d√©ploy√©: `oc get opentelemetrycollector -n observability-hub`
- V√©rifier les logs de l'otel-collector: `oc logs -n observability-hub -l app=otel-collector`
- V√©rifier la configuration de l'endpoint OTLP dans les variables d'environnement
- V√©rifier que Tempo est accessible: `oc get svc -n observability-hub | grep tempo`
- Attendre quelques secondes pour que les traces soient index√©es

### Erreur "Model not found"

**Sympt√¥mes:** Le mod√®le sp√©cifi√© n'est pas disponible

**Solutions:**
- V√©rifier les mod√®les disponibles: `oc get inferenceservice -n llama-serve`
- Utiliser le nom exact du mod√®le d√©ploy√©
- V√©rifier que le mod√®le est pr√™t: `oc get inferenceservice <nom-modele> -n llama-serve -o jsonpath='{.status.conditions}'`

## üìö Ressources

- **D√©p√¥t principal:** [rh-ai-quickstart/lls-observability](https://github.com/rh-ai-quickstart/lls-observability)
- **Documentation OpenTelemetry:** [OpenTelemetry Python](https://opentelemetry.io/docs/instrumentation/python/)
- **Documentation Tempo:** [Grafana Tempo](https://grafana.com/docs/tempo/latest/)
- **Documentation Llama Stack:** [Llama Stack Documentation](https://docs.llamastack.ai/)

## ü§ù Contribution

Les contributions sont les bienvenues ! N'h√©sitez pas √† :
- Ouvrir une issue pour signaler un bug ou proposer une am√©lioration
- Cr√©er une pull request avec vos modifications
- Partager vos cas d'usage et vos retours d'exp√©rience

## üìù Licence

Ce projet fait partie du quickstart [lls-observability](https://github.com/rh-ai-quickstart/lls-observability) et suit la m√™me licence.
