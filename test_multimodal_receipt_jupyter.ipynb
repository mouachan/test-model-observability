{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Multimodal - Extraction de ticket de caisse\n",
    "\n",
    "Ce notebook permet de tester l'extraction de produits depuis une image de ticket de caisse en utilisant Llama Stack depuis Jupyter sur OpenShift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration de l'environnement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V\u00e9rification des services disponibles\n",
    "\n",
    "Avant de lancer le test, v\u00e9rifions quels services sont disponibles dans le cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V\u00e9rifier les services disponibles\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "def check_service(name, namespace=\"llama-serve\"):\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"oc\", \"get\", \"inferenceservice\", name, \"-n\", namespace, \"-o\", \"json\"],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=10\n",
    "        )\n",
    "        if result.returncode == 0:\n",
    "            data = json.loads(result.stdout)\n",
    "            url = data.get(\"status\", {}).get(\"url\", \"N/A\")\n",
    "            ready = data.get(\"status\", {}).get(\"conditions\", [{}])[0].get(\"status\", \"Unknown\")\n",
    "            return {\"available\": True, \"url\": url, \"ready\": ready}\n",
    "    except:\n",
    "        pass\n",
    "    return {\"available\": False}\n",
    "\n",
    "print(\"\ud83d\udd0d V\u00e9rification des services disponibles:\\n\")\n",
    "\n",
    "# V\u00e9rifier llama3-2-3b\n",
    "llama3_status = check_service(\"llama3-2-3b\")\n",
    "if llama3_status[\"available\"]:\n",
    "    print(f\"\u2705 llama3-2-3b disponible: {llama3_status.get('url', 'N/A')}\")\n",
    "    print(f\"   Utilisez: export LLAMA_STACK_URL=\\\"{llama3_status.get('url', '').replace('https://', 'http://')}\\\"\")\n",
    "else:\n",
    "    print(\"\u274c llama3-2-3b non disponible\")\n",
    "    print(\"\ud83d\udca1 D\u00e9ployez-le avec:\")\n",
    "    print(\"   helm install llama3-2-3b ./helm/03-ai-services/llama3.2-3b -n llama-serve\")\n",
    "\n",
    "# V\u00e9rifier llama-stack-instance\n",
    "try:\n",
    "    result = subprocess.run(\n",
    "        [\"oc\", \"get\", \"pods\", \"-n\", \"llama-serve\", \"-l\", \"app.kubernetes.io/name=llama-stack-instance\", \"-o\", \"jsonpath={.items[0].status.phase}\"],\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        timeout=10\n",
    "    )\n",
    "    if result.returncode == 0 and result.stdout.strip() == \"Running\":\n",
    "        print(\"\\n\u2705 llama-stack-instance en cours d'ex\u00e9cution\")\n",
    "    else:\n",
    "        print(\"\\n\u274c llama-stack-instance non disponible ou en erreur\")\n",
    "        print(\"\ud83d\udca1 Utilisez directement vLLM si disponible\")\n",
    "except:\n",
    "    print(\"\\n\u26a0\ufe0f  Impossible de v\u00e9rifier llama-stack-instance\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Configuration pour OpenShift\n",
    "# Depuis Jupyter, on peut acc\u00e9der directement aux services internes\n",
    "os.environ.setdefault(\n",
    "    \"LLAMA_STACK_URL\",\n",
    "    \"http://llama-stack-instance-service.llama-serve.svc.cluster.local:8321\"\n",
    ")\n",
    "\n",
    "os.environ.setdefault(\n",
    "    \"MODEL_NAME\",\n",
    "    \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    ")\n",
    "\n",
    "os.environ.setdefault(\n",
    "    \"OTEL_TRACE_ENDPOINT\",\n",
    "    \"http://otel-collector-collector.observability-hub.svc.cluster.local:4318/v1/traces\"\n",
    ")\n",
    "\n",
    "print(f\"\u2705 Configuration:\")\n",
    "print(f\"   LLAMA_STACK_URL: {os.environ.get('LLAMA_STACK_URL')}\")\n",
    "print(f\"   MODEL_NAME: {os.environ.get('MODEL_NAME')}\")\n",
    "print(f\"   OTEL_TRACE_ENDPOINT: {os.environ.get('OTEL_TRACE_ENDPOINT')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation des d\u00e9pendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installer les d\u00e9pendances si n\u00e9cessaire\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    try:\n",
    "        __import__(package.replace('-', '_'))\n",
    "        print(f\"\u2705 {package} d\u00e9j\u00e0 install\u00e9\")\n",
    "    except ImportError:\n",
    "        print(f\"\ud83d\udce6 Installation de {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        print(f\"\u2705 {package} install\u00e9\")\n",
    "\n",
    "# Installer les d\u00e9pendances n\u00e9cessaires\n",
    "install_package(\"requests\")\n",
    "install_package(\"opentelemetry-api\")\n",
    "install_package(\"opentelemetry-sdk\")\n",
    "install_package(\"opentelemetry-exporter-otlp-proto-http\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import du script de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter le r\u00e9pertoire des tests au path si n\u00e9cessaire\n",
    "if '.' not in sys.path:\n",
    "    sys.path.insert(0, '.')\n",
    "\n",
    "# Importer les fonctions n\u00e9cessaires\n",
    "from test_multimodal_receipt import extract_products_from_receipt, setup_tracing\n",
    "\n",
    "print(\"\u2705 Script import\u00e9 avec succ\u00e8s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration du tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialiser le tracing OpenTelemetry\n",
    "tracer = setup_tracing()\n",
    "print(\"\u2705 Tracing OpenTelemetry configur\u00e9\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test avec une image de ticket de caisse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemin vers l'image (ajustez selon votre environnement)\n",
    "image_path = \"download/receipt.jpeg\"  # ou le chemin complet vers votre image\n",
    "\n",
    "# V\u00e9rifier que l'image existe\n",
    "import os\n",
    "if not os.path.exists(image_path):\n",
    "    print(f\"\u274c L'image {image_path} n'existe pas\")\n",
    "    print(\"\ud83d\udca1 Vous pouvez:\")\n",
    "    print(\"   1. T\u00e9l\u00e9charger une image avec download_receipt_image.py\")\n",
    "    print(\"   2. Uploader une image via l'interface Jupyter\")\n",
    "    print(\"   3. Utiliser une image existante dans votre workspace\")\n",
    "else:\n",
    "    print(f\"\u2705 Image trouv\u00e9e: {image_path}\")\n",
    "    print(f\"   Taille: {os.path.getsize(image_path)} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction des produits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraire les produits du ticket de caisse\n",
    "if os.path.exists(image_path):\n",
    "    try:\n",
    "        print(f\"\ud83d\udd0d Analyse du ticket de caisse: {image_path}\")\n",
    "        print(f\"\ud83d\udce1 Connexion \u00e0 Llama Stack: {os.environ.get('LLAMA_STACK_URL')}\")\n",
    "        print(f\"\ud83e\udd16 Mod\u00e8le: {os.environ.get('MODEL_NAME')}\\n\")\n",
    "        \n",
    "        result = extract_products_from_receipt(image_path)\n",
    "        \n",
    "        # Afficher les r\u00e9sultats\n",
    "        print(\"=\" * 60)\n",
    "        print(\"\ud83d\udccb R\u00c9SULTATS DE L'EXTRACTION\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        if \"store\" in result and result[\"store\"]:\n",
    "            print(f\"\ud83c\udfea Magasin: {result['store']}\")\n",
    "        \n",
    "        if \"date\" in result and result[\"date\"]:\n",
    "            print(f\"\ud83d\udcc5 Date: {result['date']}\")\n",
    "        \n",
    "        print(f\"\\n\ud83d\uded2 Produits ({len(result.get('products', []))}):\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        total_calculated = 0.0\n",
    "        for i, product in enumerate(result.get(\"products\", []), 1):\n",
    "            name = product.get(\"name\", \"N/A\")\n",
    "            price = product.get(\"price\", 0.0)\n",
    "            quantity = product.get(\"quantity\", 1)\n",
    "            \n",
    "            print(f\"{i}. {name}\")\n",
    "            print(f\"   Prix: {price:.2f} \u20ac\")\n",
    "            if quantity > 1:\n",
    "                print(f\"   Quantit\u00e9: {quantity}\")\n",
    "            \n",
    "            total_calculated += price * quantity\n",
    "            print()\n",
    "        \n",
    "        print(\"-\" * 60)\n",
    "        if result.get(\"total\"):\n",
    "            print(f\"\ud83d\udcb0 Total (extrait): {result['total']} \u20ac\")\n",
    "        print(f\"\ud83d\udcb0 Total (calcul\u00e9): {total_calculated:.2f} \u20ac\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Sauvegarder les r\u00e9sultats\n",
    "        import json\n",
    "        output_file = \"receipt_extraction_result.json\"\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(result, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"\\n\ud83d\udcbe R\u00e9sultats sauvegard\u00e9s dans: {output_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n\u274c Erreur: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affichage des r\u00e9sultats en format JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher les r\u00e9sultats en format JSON structur\u00e9\n",
    "if os.path.exists(\"receipt_extraction_result.json\"):\n",
    "    import json\n",
    "    with open(\"receipt_extraction_result.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        result = json.load(f)\n",
    "    \n",
    "    print(\"\ud83d\udcca R\u00e9sultats JSON:\")\n",
    "    print(json.dumps(result, indent=2, ensure_ascii=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}